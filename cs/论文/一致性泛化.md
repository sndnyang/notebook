Title: 一致性正则化  
Slug: consistency-regularization  
Date: 2019/8/29 22:06:23  
Authors: sndnyang sndnyang.github.io  
Category:  神经网络  
Tags: 论文, 人工智能   
Summary:   简要介绍一致性正则化方法  


[TOC]


# 摘要

深度学习神经网络甚至于多数的机器学习方法都有几个问题：

1. 易于过拟合，泛化能力不强
2. 不稳定，极小的扰动噪音就会改变模型的预测结果

本文介绍若干基于预测一致性原则的正则化方法。所谓一致性，是指输出、预测的一致性， 如前所述， 给输入添加极小的扰动后， 神经网络的预测就会发生很大的变化， 所以提高一致性后就能提高模型的泛化能力， 同时， 因为这些方法往往基于模型输出的预测向量，不需要具体的标签，所以基于一致性的正则化方法基本能应用于半监督学习。



# 核心思想

简单来说， 预测的一致性就是希望两个预测结果尽可能的接近， 数学化描述如下：

$$D [p(y | \text{Augment}(x), \mathbf{\theta}) - p(y | \text{Augment}(x), \mathbf{\theta})]$$

$D[p, q]$ 是距离的度量函数， 比如$\|p-q\|^2_2 $ 或 KL散度（KL-divergence）$\sum p_i \log \frac{p_i}{q_i}$

这里的$p(y|x, \theta)$ 和 $\text{Augment}(x)$ 会引入一定的随机性或扰动， 一般（我）把这些随机性或扰动分类如下：

1. 常规的数据增强， 如图像翻转，加随机噪音
2. 基于GAN
3. 时序移动平均
4. 同一模型多次预测时，Dropout层随机舍弃
5. 多模型
6. 对抗样本扰动Adversarial Example
7. 高级数据增强（基于强化学习或自监督学习）
8. 数据线性混合



# 基于常规的数据增强

[$\Pi$-model](https://arxiv.org/pdf/1610.02242.pdf) 

[Regularization with stochastic transformations and perturbations](https://arxiv.org/abs/1606.04586)



# 基于时序移动平均

[Temporal Ensembling](https://arxiv.org/pdf/1610.02242.pdf)

[Mean Teacher](https://arxiv.org/abs/1703.01780)

[Stochastic Weight Averaging](https://arxiv.org/abs/1806.05594)



# 基于对抗扰动

[Adversarial Training](https://arxiv.org/abs/1412.6572)

[Unified Adversarial Examples](https://arxiv.org/abs/1511.06385)

[Virtual Adversarial Training](https://arxiv.org/abs/1704.03976)

[Adversarial Dropout](https://arxiv.org/abs/1707.03631)

[Adversarial Dropout GAN](https://arxiv.org/abs/1711.01575)



# 基于高级数据增强

[unsupervised data augmentation](https://arxiv.org/abs/1904.12848)

[Self-supervised SSL](https://arxiv.org/abs/1905.03670)



# 基于数据线性混合

[Mixup](https://arxiv.org/abs/1710.09412)

[MixMatch](https://arxiv.org/abs/1905.02249)

